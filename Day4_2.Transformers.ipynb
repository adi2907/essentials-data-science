{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a transformer\n",
    "A transformer is a type of neural network architecture that excels at processing sequential data (like text, time series, or even music) by using a mechanism called \"attention\" to understand relationships between different parts of the sequence.\n",
    "\n",
    "What are Transformers Used For?\n",
    "\n",
    "Natural Language Processing (NLP):\n",
    "\n",
    "**Language Translation**\n",
    "\n",
    "**Text Generation**\n",
    "\n",
    "**Question Answering**\n",
    "\n",
    "**Text Summarization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Differences Between Transformers and Feed-Forward Neural Networks (FFNN)\n",
    "\n",
    "Feed-forward neural networks, the most basic version of neural networks, have difficulty in working with different sizes of text inputs.\n",
    "\n",
    "---\n",
    "\n",
    "**Input Representation:**\n",
    "In a feed-forward neural network (FFNN), you need to convert the entire sentence into a **fixed-size input**. This can be done by:\n",
    "- **Summing or averaging word embeddings**.\n",
    "- **Using a \"bag of words\" (BoW) representation**, where you count the occurrence of words in the sentence and feed it into an FFNN.\n",
    "\n",
    "---\n",
    "\n",
    "**Challenges with FFNN:**\n",
    "\n",
    "- **Loss of Sequence Information**:\n",
    "  - The order of words in the sentence is lost. For example, the FFNN cannot distinguish between:\n",
    "    - *\"The movie was fantastic.\"* (positive)\n",
    "    - *\"The movie was not fantastic.\"* (negative)\n",
    "\n",
    "- **Cannot Handle Variable-Length Sentences**:\n",
    "  - FFNN requires fixed-size input, so it struggles with sentences of varying lengths.\n",
    "\n",
    "- **Contextual Dependencies**:\n",
    "  - Sentiment depends on relationships between words (e.g., *\"not\"* modifies *\"fantastic\"*). FFNN cannot capture these dependencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run through the transformer architecture and how training is performed. Honestly, thats  all you need to do to perform most of the tasks unless you are building a transformer yourself and training it. Most of the NLP tasks can be done by existing pre-trained transformers which are available by the hundreds on HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture of Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 1 Embeddings\n",
    "Convert each word into embeddings and positional encoding e.g. end of sentence\n",
    "\n",
    "#### Layer 2 Multi-head attention\n",
    "\n",
    "Query (Q): Represents what \"cat\" is looking for (e.g., \"what action does the cat do?\")\n",
    "Key (K): Each word's \"advertisement\" of what information it holds\n",
    "Value (V): The actual information content each word contributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query (Q): Represents what \"cat\" is looking for (e.g., \"what action does the cat do?\")\n",
    "Key (K): Each word's \"advertisement\" of what information it holds\n",
    "Value (V): The actual information content each word contributes\n",
    "\n",
    "\n",
    "Query vectors look for specific patterns:\n",
    "\n",
    "Q(\"cat\") looks for its verb/action\n",
    "Q(\"sits\") looks for its subject\n",
    "Q(\"on\") looks for its object\n",
    "\n",
    "Key vectors advertise their roles:\n",
    "\n",
    "K(\"cat\") advertises \"I am a subject noun\"\n",
    "K(\"sits\") advertises \"I am an action verb\"\n",
    "K(\"mat\") advertises \"I am an object noun\"\n",
    "\n",
    "Value vectors contain the actual semantic content:\n",
    "\n",
    "V(\"cat\") contains features about \"feline, animal, subject\"\n",
    "V(\"sits\") contains features about \"sitting action, present tense\"\n",
    "V(\"mat\") contains features about \"floor covering, object\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when the model processes \"cat\":\n",
    "\n",
    "Its Query vector will have high dot product with the Key vector of \"sits\" (because cats do actions)\n",
    "This high attention score means it will incorporate more of the Value vector from \"sits\"\n",
    "The Value vector contains the actual semantic information about sitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 3 Feed Forward Neural network\n",
    "\n",
    "**After attention for \"cat\":**\n",
    "- Has information about \"sits\" (verb relationship)\n",
    "- Has information about \"the\" (article relationship)\n",
    "\n",
    "**FFN then**\n",
    "1. Expands this combined context (W1)\n",
    "2. Applies non-linearity (ReLU)\n",
    "3. Transforms it into useful features (W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adityaganguli/.pyenv/versions/3.8.16/envs/tech-env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import torch\n",
    "\n",
    "# Load BART model and tokenizer once\n",
    "model_name = \"facebook/bart-large\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation:\n",
      "I have a dream for you.Posted by\n"
     ]
    }
   ],
   "source": [
    "# Load BART model and tokenizer once\n",
    "model_name = \"facebook/bart-large\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def generate_text(prompt, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=max_length,\n",
    "        min_length=10,\n",
    "        temperature=0.2,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "prompt = \"I have a dream that\"\n",
    "print(\"\\nGeneration:\")\n",
    "print(generate_text(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarization:\n",
      "The Internet    The Internet is a global network of interconnected computers that use standardized communication protocols   .   to share resources and information. It emerged from ARPANET\n"
     ]
    }
   ],
   "source": [
    "def summarize_text(text, max_length=40, min_length=20):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "        length_penalty=2.0,\n",
    "        num_beams=4\n",
    "    )\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "text = \"\"\"\n",
    "    The Internet is a global network of interconnected computers that use standardized communication protocols \n",
    "    to share resources and information. It emerged from ARPANET in the late 1960s as a project of the United \n",
    "    States Department of Defense. Today, it connects billions of devices worldwide and has revolutionized \n",
    "    communication, commerce, entertainment, and countless other aspects of modern life.\n",
    "    \"\"\"\n",
    "print(\"\\nSummarization:\")\n",
    "print(summarize_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question Answering:\n",
      " Anthropic\n",
      "\n",
      "Context: The Eiffel Tower was constructed between 1887 and 1889 and was designed by engineer Gustave Eiffel.\n",
      "Question: When was the Eiffel Tower built?\n",
      "Answer:  between 1887 and 1889\n",
      "\n",
      "Context: Python was created by Guido van Rossum and was first released in 1991. It was named after the TV show Monty Python's Flying Circus.Nehru became the prime minister of India after India gained independence in 1947\n",
      "Question: Why was Python named Python?Who was the first prime minister of India?\n",
      "Answer: Nehru\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load RoBERTa model and tokenizer\n",
    "model_name = \"deepset/roberta-base-squad2\"  # This is fine-tuned on SQuAD dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "def answer_question(context, question):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(\n",
    "        question,\n",
    "        context,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    # Get model outputs\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # Get the most likely beginning and end of answer\n",
    "    answer_start = torch.argmax(outputs.start_logits)\n",
    "    answer_end = torch.argmax(outputs.end_logits)\n",
    "    \n",
    "    # Convert tokens back to string\n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(\n",
    "            inputs[\"input_ids\"][0][answer_start:answer_end + 1]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Test examples\n",
    "context = \"Claude is an AI assistant created by Anthropic in 2022.\"\n",
    "question = \"Who created Claude?\"\n",
    "print(\"\\nQuestion Answering:\")\n",
    "print(answer_question(context, question))\n",
    "\n",
    "# More test examples\n",
    "contexts = [\n",
    "    \"The Eiffel Tower was constructed between 1887 and 1889 and was designed by engineer Gustave Eiffel.\",\n",
    "    \"Python was created by Guido van Rossum and was first released in 1991. It was named after the TV show Monty Python's Flying Circus.\"\n",
    "    \"Nehru became the prime minister of India after India gained independence in 1947\"\n",
    "]\n",
    "\n",
    "questions = [\n",
    "    \"When was the Eiffel Tower built?\",\n",
    "    \"Why was Python named Python?\"\n",
    "    \"Who was the first prime minister of India?\"\n",
    "]\n",
    "\n",
    "for context, question in zip(contexts, questions):\n",
    "    print(\"\\nContext:\", context)\n",
    "    print(\"Question:\", question)\n",
    "    print(\"Answer:\", answer_question(context, question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning an existing pre-trained transformer\n",
    "\n",
    "Finetuning is especially useful for NLP tasks like text generation to get up to date on information, answer questions correctly or adopt a certain style. Here we will use the BART transformer to learn the shakespearean way of speaking and let it generate text in that manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1\n"
     ]
    }
   ],
   "source": [
    "import accelerate\n",
    "print(accelerate.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartForConditionalGeneration, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# Set device to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"facebook/bart-base\"  # Using smaller model for faster training\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Load Shakespeare text\n",
    "with open('shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    shakespeare_text = f.read().split('\\n')\n",
    "\n",
    "# Create dataset\n",
    "dataset = Dataset.from_dict({\n",
    "    'text': shakespeare_text\n",
    "})\n",
    "\n",
    "# Preprocessing function to add inputs and labels\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=128\n",
    "    )\n",
    "    # Use the same input as label for reconstruction training\n",
    "    inputs['labels'] = inputs['input_ids'].copy()\n",
    "    return inputs\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "# Training arguments for CPU\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./shakespeare-bart\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,  # Reduced batch size for CPU\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=500,\n",
    "    logging_steps=100,\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=False,  # Disable mixed precision for CPU\n",
    "    optim=\"adamw_torch\",\n",
    "    no_cuda=True  # Explicitly disable GPU\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./shakespeare-bart-finetuned\")\n",
    "tokenizer.save_pretrained(\"./shakespeare-bart-finetuned\")\n",
    "\n",
    "# Function to test text generation\n",
    "def generate_shakespeare(prompt, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=128, truncation=True).to(device)\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=max_length,\n",
    "        num_beams=5,\n",
    "        temperature=0.7,\n",
    "        no_repeat_ngram_size=2,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Test generation with various prompts\n",
    "test_prompts = [\n",
    "    \"To be, or not to be\",\n",
    "    \"The course of true love\",\n",
    "    \"All the world's a\",\n",
    "    \"What light through yonder\",\n",
    "]\n",
    "\n",
    "print(\"\\nTesting generation with various prompts:\")\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"Generated text:\")\n",
    "    print(generate_shakespeare(prompt))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tech-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
