{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language processing\n",
    "\n",
    "Natural Language Processing (NLP) is a field of AI that helps computers understand, interpret, and respond to human language.  It has applications in chatbots, sentiment analysis, language translation, and more.\n",
    "\n",
    "Challenges in NLP\n",
    "\n",
    "Ambiguity: Words can have multiple meanings (e.g., \"bank\" as a financial institution vs river bank).\n",
    "\n",
    "Context Understanding: Understanding implied meanings or references. E.g. 'Digging one a hole'\n",
    "\n",
    "Computational Complexity: Analyzing large datasets efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be using standard libraries which have been trained on large corpus of data to get a lot of meaning directly from the sentences. E.g. spaCy is commonly used in all production environements for most NLP tasks like named entity recognition, stemming/lemmatization, sentiment analysis etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy==3.5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization:\n",
    "Splitting text into individual words or sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens  [Manufacturing, machines, are, important, ., Machines, help, production]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "text = \"Manufacturing machines are important. Machines help production\"\n",
    "doc = nlp(text)\n",
    "tokens = []\n",
    "for token in doc:\n",
    "    tokens.append(token)\n",
    "print(\"Tokens \",tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "Stemming: Reduces words to their root form by chopping off suffixes.\n",
    "Lemmatization: Converts words to their dictionary base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['manufacturing', 'machine', 'be', 'important', '.', 'machine', 'help', 'production']\n"
     ]
    }
   ],
   "source": [
    "# In spaCY, stemming is not supported and one can directly get the root word by lemmatization\n",
    "lemmas = [token.lemma_ for token in tokens]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Industry Names: ['production machine', 'manufacturing machine', 'machine', 'machining']\n"
     ]
    }
   ],
   "source": [
    "industries = [\"Machines\", \"Machine\", \"Machining\", \"Production Machines\", \"Manufacturing machine\"]\n",
    "\n",
    "normalized_industries = []\n",
    "for industry in industries:\n",
    "    doc = nlp(industry)\n",
    "    lemmatized_text = \" \".join([token.lemma_ for token in doc])\n",
    "    normalized_industries.append(lemmatized_text)\n",
    "\n",
    "print(\"Normalized Industry Names:\", list(set(normalized_industries)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition and Parts of speech tagging\n",
    "\n",
    "NER (Named Entity Recognition): Identifies real-world entities like people, organizations, locations, etc., in text (e.g., \"Narendra Modi\" → PERSON, \"India\" → GPE). spaCy uses statistical models trained on labeled data to detect and label these entities.\n",
    "\n",
    "POS Tagging (Part of Speech Tagging): Assigns grammatical categories to words, such as noun, verb, or adjective (e.g., \"run\" → VERB). spaCy performs this using pre-trained language models that analyze word context.\n",
    "\n",
    "spaCy processes text by building a \"Doc\" object containing linguistic annotations for each token, including its lemma, POS tag, and named entity label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens and POS tags\n",
      "Narendra: PROPN\n",
      "Modi: PROPN\n",
      "is: AUX\n",
      "the: DET\n",
      "Prime: PROPN\n",
      "Minister: PROPN\n",
      "of: ADP\n",
      "India: PROPN\n",
      "\n",
      " Named Entities\n",
      "Narendra Modi: PERSON\n",
      "India: GPE\n"
     ]
    }
   ],
   "source": [
    "text = \"Narendra Modi is the Prime Minister of India\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"Tokens and POS tags\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text}: {token.pos_}\")\n",
    "\n",
    "print(\"\\n Named Entities\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text}: {ent.label_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before transformers, embeddings etc came into the picture, these were the libraries which helped us with a variety of NLP tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.3/626.3 KB\u001b[0m \u001b[31m614.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.8 in /Users/adityaganguli/.pyenv/versions/3.8.16/envs/tech-env/lib/python3.8/site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: joblib in /Users/adityaganguli/.pyenv/versions/3.8.16/envs/tech-env/lib/python3.8/site-packages (from nltk>=3.8->textblob) (1.4.2)\n",
      "Requirement already satisfied: tqdm in /Users/adityaganguli/.pyenv/versions/3.8.16/envs/tech-env/lib/python3.8/site-packages (from nltk>=3.8->textblob) (4.67.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/adityaganguli/.pyenv/versions/3.8.16/envs/tech-env/lib/python3.8/site-packages (from nltk>=3.8->textblob) (2024.11.6)\n",
      "Requirement already satisfied: click in /Users/adityaganguli/.pyenv/versions/3.8.16/envs/tech-env/lib/python3.8/site-packages (from nltk>=3.8->textblob) (8.1.8)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.18.0.post0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Users/adityaganguli/.pyenv/versions/3.8.16/envs/tech-env/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love spaCy and Python! They make NLP easy and fun.\n",
      "Polarity: 0.4527777777777778 (range -1 to 1, negative to positive)\n",
      "Subjectivity: 0.5444444444444444 (range 0 to 1, objective to subjective)\n",
      "Text: I hate waiting for hours in traffic.\n",
      "Polarity: -0.8 (range -1 to 1, negative to positive)\n",
      "Subjectivity: 0.9 (range 0 to 1, objective to subjective)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load English language model in spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    # Process text using spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Convert processed text to string and analyze sentiment using TextBlob\n",
    "    sentiment = TextBlob(doc.text).sentiment\n",
    "    \n",
    "    # Output sentiment polarity and subjectivity\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Polarity: {sentiment.polarity} (range -1 to 1, negative to positive)\")\n",
    "    print(f\"Subjectivity: {sentiment.subjectivity} (range 0 to 1, objective to subjective)\")\n",
    "\n",
    "# Example usage\n",
    "analyze_sentiment(\"I love spaCy and Python! They make NLP easy and fun.\")\n",
    "analyze_sentiment(\"I hate waiting for hours in traffic.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Embeddings\n",
    "Alright, things are going to get interesting. So for a long time, researchers used feature engineering using different parts of speech, sentiment analysis etc. In came the word2vec model where words or tokens began to be represented as vectors. Introduced by Google researchers led by Tomas Mikolov, Word2Vec revolutionized NLP by learning dense word embeddings in a vector space. This allowed the model to capture semantic relationships (e.g., \"king - man + woman ≈ queen\").\n",
    "\n",
    "Vectors are in n dimensional space. Here's a simple representation of a word vector in few dimensions\n",
    "![Word vectors](https://corpling.hypotheses.org/files/2018/04/3dplot-500x381.jpg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word vectors are built around the concept of \"meaning of a word can be derived from the company it keeps\". Hence a neural network. Hence, a neural network is trained to predict either the target word based on the surrounding words (CBOW) or the surrounding words based on the target word (Skip-Gram) \n",
    "\n",
    "\n",
    "![CBOW Skip gram](https://i0.wp.com/spotintelligence.com/wp-content/uploads/2023/12/continuous-bag-of-words-vs-skip-gram-1-1024x576.webp?resize=1024%2C576&ssl=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at some examples of how one can derive meaning out of word vectors. We will be using pre-trained models since they have been trained on a large corpus of text. Stanford NLP group has a good pre-trained word vector model called Glove. We will need to download the Glove word vectors from http://nlp.stanford.edu/data/glove.6B.zip. then unzip and link it to the model path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "glove_path = \"/Users/adityaganguli/Downloads/glove/glove.6B.100d.txt\"\n",
    "print(\"File exists:\", os.path.exists(glove_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How word embeddings are trained using neural networks\n",
    "\n",
    "Training Process:\n",
    "\n",
    "**Input Layer**: A one-hot encoded vector for the word.\n",
    "\n",
    "**Hidden Layer**: Contains the embedding weights. This layer maps high-dimensional one-hot vectors to dense, lower-dimensional embeddings.\n",
    "\n",
    "**Output Layer**: Predicts the probability of context words (softmax activation).\n",
    "\n",
    "**Loss Function**: Optimizes based on how well the model predicts the surrounding words (e.g., cross-entropy loss).\n",
    "\n",
    "#### Optimization:\n",
    "\n",
    "**Backpropagation** adjusts the weights (embeddings) based on the loss, gradually improving the vector representation of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_model(file_path):\n",
    "    glove_model = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.split()\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype='float32')\n",
    "            glove_model[word] = vector\n",
    "    print(f\"Loaded {len(glove_model)} word vectors.\")\n",
    "    return glove_model\n",
    "\n",
    "# Specify GloVe file path (100d)\n",
    "glove_path = \"/Users/adityaganguli/Downloads/glove/glove.6B.100d.txt\"\n",
    "glove_model = load_glove_model(glove_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "France - Paris + Berlin ≈ germany\n",
      "Germany - Hitler + Italy ≈ mussolini\n",
      "Dog - Puppy + Cat ≈ puppy\n",
      "Paris - Eiffel Tower + delhi ≈ maldives\n"
     ]
    }
   ],
   "source": [
    "# Function to find closest word based on vector arithmetic\n",
    "def find_closest_word(glove_model, word_vec):\n",
    "    closest_word = None\n",
    "    min_dist = float(\"inf\")\n",
    "    for word, vec in glove_model.items():\n",
    "        dist = np.linalg.norm(word_vec - vec)\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            closest_word = word\n",
    "    return closest_word\n",
    "\n",
    "# Vector arithmetic examples:\n",
    "\n",
    "# 1. Country Capitals: Germany - Berlin + France ≈ ?\n",
    "capital_prediction = find_closest_word(glove_model, glove_model[\"france\"] - glove_model[\"paris\"] + glove_model[\"berlin\"])\n",
    "print(f\"France - Paris + Berlin ≈ {capital_prediction}\")\n",
    "\n",
    "# 2. Country Leaders: Germany - Hitler + Italy ≈ ?\n",
    "leader_prediction = find_closest_word(glove_model, glove_model[\"italy\"] - glove_model[\"germany\"] + glove_model[\"hitler\"])\n",
    "print(f\"Germany - Hitler + Italy ≈ {leader_prediction}\")\n",
    "\n",
    "# 3. Animal Babies: Dog - Puppy + Cat ≈ ?\n",
    "baby_animal_prediction = find_closest_word(glove_model, glove_model[\"cat\"] - glove_model[\"dog\"] + glove_model[\"puppy\"])\n",
    "print(f\"Dog - Puppy + Cat ≈ {baby_animal_prediction}\")\n",
    "\n",
    "# 4. Famous Landmarks: Paris - Eiffel Tower + India ≈ ?\n",
    "landmark_prediction = find_closest_word(glove_model, glove_model[\"india\"] - glove_model[\"paris\"] + glove_model[\"eiffel\"])\n",
    "print(f\"Paris - Eiffel Tower + delhi ≈ {landmark_prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'king' - 'queen' + 'woman' ≈ man\n"
     ]
    }
   ],
   "source": [
    "result_vector = glove_model[\"king\"] - glove_model[\"queen\"] + glove_model[\"woman\"]\n",
    "closest_word = find_closest_word(glove_model, result_vector)\n",
    "print(f\"'king' - 'queen' + 'woman' ≈ {closest_word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vectors were revolutionary because they\n",
    "#### Semantic Representation: \n",
    "Word vectors encode meaning, placing similar words (e.g., \"king\" and \"queen\") close in a continuous vector space.\n",
    "#### Efficient Encoding: \n",
    "Replaced sparse one-hot encoding with dense, low-dimensional vectors, reducing computational and storage costs.\n",
    "#### Analogy Reasoning: \n",
    "Enabled analogies like showcasing semantic relationships like king-queen = man-woman\n",
    "#### Scalability: \n",
    "Trained on large, unlabeled datasets, making them broadly applicable for diverse NLP tasks.\n",
    "#### Foundation for Modern NLP:\n",
    "Inspired contextual embeddings (e.g., BERT, GPT), leading to breakthroughs in machine translation, chatbots, and search engines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tech-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
