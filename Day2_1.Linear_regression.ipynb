{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "Regression is the OG Machine learning algorithm. Linear regression and different derivatives are widely used today in solving many different types of problems like forecasting, prediction etc to a high level of accuracy. It is a statistical method used to model relationships between variables. We'll cover both linear regression for continuous outcomes and logistic regression for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "$$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n $$\n",
    "\n",
    "y: dependent variable output\n",
    "x: independent variable input\n",
    "b0 is the error term\n",
    "b is the curve of the slope for different input variables\n",
    "\n",
    "### Intuition:\n",
    "**Goal:** Find the values of b0 and b1,b2 that minimize the error between predicted y and actual y\n",
    "\n",
    "**Example:** Predict steel price y based on grade x1 and thickness x2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple explanation\n",
    "\n",
    "Grade(x1)  Thickness(x2)  Price(y)\n",
    "   100         5            500\n",
    "   100         10           550\n",
    "   200         5            700\n",
    "\n",
    "Our equation becomes\n",
    "\n",
    "y = b0 + b1*x1 + b2*x2\n",
    "\n",
    "500 = b0 + b1(100) + b2(5)    # first point\n",
    "550 = b0 + b1(100) + b2(10)   # second point\n",
    "700 = b0 + b1(200) + b2(5)    # third point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subtract equation 1 from 2 to eliminate b0:\n",
    "\n",
    "550 - 500 = b1(100-100) + b2(10-5)\n",
    "50 = 5b2\n",
    "b2 = 10        # This means price increases $10 per mm of thickness\n",
    "\n",
    "700 - 500 = b1(200-100) + b2(5-5)\n",
    "200 = 100b1\n",
    "b1 = 2         # This means price increases $2 per grade unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plug back into any equation to find b0:\n",
    "\n",
    "500 = b0 + 2(100) + 10(5)\n",
    "500 = b0 + 200 + 50\n",
    "b0 = 250\n",
    "\n",
    "Our final equation\n",
    "#### Price = 250 + 2*Grade + 10*Thickness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use a data set to illustrate linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas matplotlib scikit-learn seaborn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and pre-processing\n",
    "First, let's create our healthcare dataset and prepare it for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('insurance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'sex', 'bmi', 'children', 'smoker', 'region', 'charges'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step is to examine the data available\n",
    "print(f\"Columns of dataset are {df.columns}\")\n",
    "\n",
    "# Get an overview of summary statistics\n",
    "print(df.describe())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate analysis\n",
    "Observe the distribution and count of different variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates histograms for the numerical columns age, sex, bmi, and children with 20 bins and a figure size of (10, 8).\n",
    "plt.show(): Displays the histograms.\n",
    "sns.countplot(x=col, data=df): Creates a count plot showing the frequency of unique values for each categorical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[['age','sex','bmi','children']].hist(bins=20,figsize=(10,8))\n",
    "plt.show()\n",
    "\n",
    "for col in ('sex','smoker','region'):\n",
    "    sns.countplot(x=col,data = df)\n",
    "    plt.title(f'Count plot for {col}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate analysis\n",
    "Observe the correlation among various variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try to find the correlation matrix - corr among various variables \n",
    "corr = df.corr()\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are getting an error because correlation works only among numeric columns\n",
    "# Selecting only the numeric columns\n",
    "numeric_cols = df.select_dtypes(include='number')\n",
    "corr = numeric_cols.corr()\n",
    "\n",
    "'''visualize the correlation matrix corr, with annotated correlation values\n",
    " and a color gradient from blue (negative) to red (positive)\n",
    " the vmin and vmax values are for negative to positive correlation\n",
    "'''\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Heatmap (Numeric Columns Only)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However we don't want to leave out non numeric variables in correlation analysis \n",
    "This brings us to categorical variables which have discrete number of values. The way to transform them is to convert the categorical values to numeric which the model can implicitly understand\n",
    "\n",
    "We have 2 options \n",
    "1. Label encoder. : Converts each category into a unique integer. e.g. 'male' -> 0, 'female' ->1. This is easy to implement and works for binary or small categories but can introduce order where none exists\n",
    "2. One-hot encoding: Creates new columns for each category and assigns 1 if the cateogory is present, else 0. e.g. for 'sex', if its male, it'll assign 'sex_male' to 1 and 'sex_female' to 0\n",
    "\n",
    "Now lets transform the categorical variables 'sex', 'region', 'smoker' and 'children'. For simplicity we 'll use label encoder as none of them have a lot of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "for col in ['sex', 'region', 'smoker','children']:\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "print(df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets get the entire correlation heatmap\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr,annot=True,cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "Normalization is a process of scaling numerical values to a specific range (usually [0, 1]). It is often used when\n",
    "\n",
    "Different numeric columns have very different ranges (e.g., age ranges from 18–65, charges ranges from 1000–50000).\n",
    "Models that rely on distance-based calculations (like regression, KNN, etc.) may be biased towards columns with larger ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Normalization:\n",
      "               age          bmi       charges\n",
      "count  1338.000000  1338.000000   1338.000000\n",
      "mean     39.207025    30.663397  13270.422265\n",
      "std      14.049960     6.098187  12110.011237\n",
      "min      18.000000    15.960000   1121.873900\n",
      "25%      27.000000    26.296250   4740.287150\n",
      "50%      39.000000    30.400000   9382.033000\n",
      "75%      51.000000    34.693750  16639.912515\n",
      "max      64.000000    53.130000  63770.428010\n"
     ]
    }
   ],
   "source": [
    "numeric_cols = ['age', 'bmi', 'charges']\n",
    "\n",
    "# Display statistics before normalization\n",
    "print(\"Before Normalization:\")\n",
    "print(df[numeric_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df_norm = df.copy() # Create a copy for normalized dataframe\n",
    "# fit_transform: Calculates the minimum and maximum values for each column (from fit()) and scales the values (from transform()) to a 0-1 range.\n",
    "df_norm[numeric_cols] = scaler.fit_transform(df_norm[numeric_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform back to dataframe as df_norm is a numpy array\n",
    "df_norm = pd.DataFrame(df_norm)\n",
    "print(\"After Normalization:\")\n",
    "print(df_norm.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression \n",
    "Finally we can proceed with the regression. Here we will use a library which is widely used for all statistical and ML algos. Scikit Learn. As with any machine learning problem, we will need to split the data into training set on which the model will learn the variables and the test set on which the model accuracy will be tested. \n",
    "\n",
    "You will see how within a few lines of code, the entire regression can be done. Also we can use the same dataset for other types of regression models which we will cover later but a short overview will be given here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Get the error computation code which will be used to minimise the error by the model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# We need to use the independent variables in the X-matrix and dependent variable in the Y matrix\n",
    "y=df_norm['charges']\n",
    "X = df_norm.drop(columns =['charges','smoker']) # Why did we drop 'smoker'?\n",
    "\n",
    "# Split the data into 80-20 among training and test set\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression() # Create an instance of linear regression model\n",
    "lr.fit(X_train,y_train) # X_train is features and y_train is target value\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Variable Dependence (Coefficients)\n",
    "Linear regression provides coefficients for each feature, showing their impact on the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of feature names and their coefficients\n",
    "# X.columns: The feature names (column names of X).\n",
    "# lr.coef_: The corresponding coefficients learned by the LinearRegression model after training.\n",
    "coefficients = pd.DataFrame({'Feature': X.columns, 'Coefficient': lr.coef_}).sort_values(by='Coefficient', ascending=False)\n",
    "print(coefficients)\n",
    "\n",
    "# Visualize feature importance\n",
    "sns.barplot(x='Coefficient', y='Feature', data=coefficients)\n",
    "plt.title('Feature Importance in Linear Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small Digression: How can you easily create barcharts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "df = pd.DataFrame({\n",
    "    'category': ['A', 'B', 'C', 'D'],\n",
    "    'value': [10, 15, 7, 40]\n",
    "})\n",
    "\n",
    "# Barplot\n",
    "sns.barplot(x='category', y='value', data=df, palette='Blues')\n",
    "#plt.yscale('log')\n",
    "plt.title('Barplot of Categories vs Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error (MSE) and Mean Absolute Error (MAE)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Mean Squared Error (MSE)**\n",
    "The Mean Squared Error (MSE) is defined as:\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "- \\( y_i \\): Actual value.\n",
    "- \\( \\hat{y}_i \\): Predicted value.\n",
    "- \\( n \\): Number of data points.\n",
    "\n",
    "**Key Points:**\n",
    "- MSE calculates the **average squared difference** between actual and predicted values.\n",
    "- The squaring penalizes large errors more, making MSE sensitive to large outliers.\n",
    "- The output is in **squared units** of the target variable.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Mean Absolute Error (MAE)**\n",
    "The Mean Absolute Error (MAE) is defined as:\n",
    "$$\n",
    "MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "- \\( |y_i - \\hat{y}_i| \\): Absolute difference between actual and predicted values.\n",
    "\n",
    "**Key Points:**\n",
    "- MAE calculates the **average absolute difference** between actual and predicted values.\n",
    "- MAE is more **robust to outliers** compared to MSE.\n",
    "- The output is in the **same units** as the target variable.\n",
    "\n",
    "---\n",
    "\n",
    "### **Differences Between MSE and MAE**\n",
    "\n",
    "| **Metric** | **Description**                  | **Sensitivity to Outliers** | **Units**          |\n",
    "|-------------|-----------------------------------|----------------------------|-------------------|\n",
    "| **MSE**     | Average of squared errors         | High (penalizes large errors) | Squared units (e.g., $^2$) |\n",
    "| **MAE**     | Average of absolute errors        | Low (treats all errors equally) | Same units as target (e.g., $) |\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion:**\n",
    "- Use **MSE** when you want to penalize large errors more heavily (e.g., cost-sensitive applications).\n",
    "- Use **MAE** when you want a simpler, interpretable metric that is not as sensitive to outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding errors using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "y_true = np.array([100, 120, 80])  # Actual values\n",
    "y_pred = np.array([90, 130, 75])   # Predicted values\n",
    "\n",
    "# Calculate MAE manually\n",
    "mae = np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "# Calculate MSE manually\n",
    "mse = np.mean((y_true - y_pred)**2)\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"MSE: {mse:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coming back to our regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 0.033253598577450326\n",
      "Mean Absolute Error (MAE): 0.14544994321069074\n"
     ]
    }
   ],
   "source": [
    "# Calculate errors\n",
    "mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error\n",
    "mae = mean_absolute_error(y_test, y_pred)  # Mean Absolute Error\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember charges were normalized\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.histplot(y_test, kde=True)\n",
    "plt.title('Distribution of Charges (Test Data)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Now lets see how having  'smoker' as a independent variable impacts the error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: Look at this only once you have done the exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Results (with `smoker`):\n",
      "Mean Squared Error (MSE): 0.0086\n",
      "Mean Absolute Error (MAE): 0.0668\n",
      "R-squared Score (R²): 0.7833\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Features (X) and Target (y)\n",
    "X = df_norm.drop(columns=['charges'])  # Include all features (including normalized `smoker`)\n",
    "y = df_norm['charges']  # Target variable (normalized `charges`)\n",
    "\n",
    "# Train-test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit the linear regression model\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Calculate errors\n",
    "mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error\n",
    "mae = mean_absolute_error(y_test, y_pred)  # Mean Absolute Error\n",
    "\n",
    "\n",
    "print(f\"Linear Regression Results (with `smoker`):\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Quickly lets check how Random Forest performs here on the same dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regression Results (Normalized `charges`):\n",
      "Mean Squared Error (MSE): 0.0378\n",
      "Mean Absolute Error (MAE): 0.1493\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Features (X) and Target (y)\n",
    "X = df_norm.drop(columns=['charges', 'smoker'])  # All features except the target\n",
    "y = df_norm['charges']  # Normalized charges (target variable)\n",
    "\n",
    "# Train-test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit the Random Forest model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Calculate errors\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)  # Mean Squared Error\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)  # Mean Absolute Error\n",
    "\n",
    "# Print results\n",
    "print(f\"Random Forest Regression Results (Normalized `charges`):\")\n",
    "print(f\"Mean Squared Error (MSE): {mse_rf:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_rf:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tech-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
